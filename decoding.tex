Once the error syndrome is known, it is necessary to perform a decoding process
to obtain the best guess for what error occurred. Optimally decoding such
syndromes is an NP-complete problem, and therefore no efficient (polynomial
time) algorithm yet exists \cite{Berlekamp}. However, the main challenge still
consists in obtaining a fast yet precise algorithm, which is preferably scalable
with the number of data qubits. As mentioned, book-keeping errors at the
software level allows for corrections to be applied at the end of the
computation process by adapting the measurement output of the data qubits.

We would like to mention two different approaches. The \textit{maximum
  likelihood method} \cite{Varsamopoulos_2020} finds and applies the most probable
correction consistent with a given error model. It features high accuracy but
low efficiency. Another popular approach uses the \textit{minimum weight perfect
  matching} \cite{fowler2013minimum} algorithm. In this case, error syndromes
are represented as a weighted graph. Afterwards, the vertices of this graph are
paired minimizing their total weight. This pairing establishes the operation
necessary to correct the error. The minimum weight perfect
matching protocol greatly increases decoding speed at a modest cost for the
accuracy.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "QEC_paper"
%%% End:
