In the basic theory of quantum error correction, quantum states are encoded into
several highly entangled physical qubits. By performing the appropriate parity
checks, it is possible to probe the state of the physical qubits without
changing the state of the encoded (\textit{logical}) qubit, in order to detect
errors on individual physical qubits. These parity checks are the
\textit{stabilizers} of the error correction code; when all stabilizer
measurements return an eigenvalue of $+1$, it signals that no error has occurred
\cite{nielsen_chuang_2010}. Effective encoding protocols are constructed such
that different error processes give rise to unique \textit{syndromes}, which
are the list of stabilizer measurement outcomes providing the information
needed to correct the error \cite{fowler12_surfac_codes}.

The error detection scheme implemented by the surface code, shown in Fig.
\ref{fig:surface_code}, is capable of precisely identifying errors in large
arrays of qubits. The $X$- and $Z$-stabilizer ancillas conduct parity
measurements between the data qubits to which they are connected through a
sequence of controlled-not (CNOT) operations. Should an error occur on a given
data qubit, depending on the error type ($X$, $Y$, or $Z$) either the $X$ or $Z$
(or both) type stabilizers will fire (return an outcome of $-1$ in their
measurement), which heralds the error and allows for its appropriate correction.

\subsection{Fault tolerance}
In any real implementation of physical qubits, there are several sources of
error. State preparation and measurement may be imperfectly carried out.
Coherent errors also occur through the imperfect application of quantum gates in
circuits \cite{Devitt_2013}. Error models and error correction schemes are all
predicated on the assumption that low-weight errors (affecting only a few
qubits) are more likely than high weight errors. However, errors occurring at
any of these steps may propagate through the different operations to the rest of
the components of the system. This could lead to cascading errors throughout the
entire process, destroying all reliability in the computation. If a quantum
computer is ever to be realized, errors must be carefully contained.

In this context of error propagation the concept of \textit{fault-tolerant}
quantum computing arises. A certain operation is said to be fault-tolerant if a
single error occurring at any step between two QEC cycles causes at most one
error at each of the logical blocks involved in the operation
\cite{Devitt_2013}. An example of fault-tolerant operation is shown in Fig.
\ref{fig:fault_tol} (b) where a bit-flip error in the top logical block
propagates only once to the bottom logical block (in contrast to (a) where it
propagates multiply). It is noteworthy that the constraint of only one error
propagating to different logical blocks can be relaxed when increasing the code
\textit{distance} \cite{Devitt_2013}, where the distance is defined as the
smallest number of physical gates required to perform a logical level operation.
In fact, for codes of distance $d$, it suffices to require $\lfloor{(d-1)/2}
\rfloor$ errors at each logical block.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{images/fault_tolerance.pdf}
  \caption{Exmple of an operation that is not fault-tolerant (a) and one that is
    fault-tolerant (b). In these examples a bit-flip error in the top logical
    block propagates through the CNOT gates to the bottom logical block. Figure
    from \cite{Devitt_2013}.}
  \label{fig:fault_tol}
\end{figure}

\subsection{Decoding Algorithms}
Once the error syndrome is known, it is necessary to perform a decoding process
to obtain the best guess for what error occurred. Optimally decoding such
syndromes is an NP-complete problem, and therefore no efficient (polynomial
time) algorithm yet exists \cite{Berlekamp}. However, the main challenge still
consists in obtaining a fast yet precise algorithm, which is preferably scalable
with the number of data qubits. It should be noted that errors can simply be
tracked at the software level allowing for corrections to be applied at the end
of the computation process by adapting the measurement output of the data
qubits.

We would like to mention two different decoding algorithms although new and
promising approaches are also being explored \cite{Varsamopoulos_2020}. The
\textit{maximum likelihood method} finds and applies the most probable
correction consistent with a given error model. It features high accuracy but
low efficiency. Another popular approach uses the \textit{minimum weight perfect
  matching} \cite{fowler2013minimum} algorithm. In this case, error syndromes
are represented as a weighted graph. Afterwards, the vertices of this graph are
paired minimizing their total weight. This pairing establishes the operation
necessary to correct the error. The minimum weight perfect matching protocol
greatly increases decoding speed at a modest cost for accuracy.

\subsection{Error threshold}
One great strength of the surface code is its ability to diagnose multiple
errors simultaneously. Should two adjacent $X$-checks fire at one end of the
surface, while two $Z$-checks fire at the other end, we can conclude that a
phase-flip error occurred on the data qubit between the two $X$-checks, while a
bit-flip error occurred between the $Z$-checks. One must be careful, however,
because decoding the error syndrome for any code is a problem without a unique
solution \cite{terhal15}. Taking the top-left data qubit in Fig.
\ref{fig:surface_code} as an example, one can confirm that a bit-flip error on
that qubit would give rise to the exact same syndrome as a series of bit-flip
errors on the rest of the top row of data qubits. For systems with small error
probabilities, the single bit-flip error is of course much more likely than the
long series of errors required to produce the same syndrome, but as the error
rate grows, such mistakes in identifying the cause of a syndrome become more
commonplace.

When a syndrome is misinterpreted, the code implements the \textit{wrong
  correction}, and this can lead to a logical level operation being applied to
the logical qubit. The rate at which this occurs, called the \textit{logical
  error rate}, as a function of the physical error rate of the code is of great
interest in characterizing the effectiveness of an error correction protocol.

Indeed, we want the logical error rate to decrease as a function of increasing
code distance, either by means of concatenation or, in the case of surface
codes, by increasing the dimensions of surface. However, a high error rate would
prevent this from happening. In the case of surfaces codes, a simple empirical
equation that encapsulates the main properties of the logical error rate
obtained in simulations is \cite{fowler12_surfac_codes}
\begin{equation}
  \label{eq:1}
  P_L = c\left(\frac{p}{p_{th}}\right)^{\frac{ d+1 }{2}},
\end{equation}
where $d$ is the distance of the QEC protocol employed, $c$ is a constant that
depends on the exact characteristics of the error model, $p$ is the physical
error rate under sensible assumptions and $p_{th}$ is the threshold error rate.
This equation clearly shows that the process of encoding is beneficial (leads to
a lower logical error rate) only if the physical qubit error rate is below the
critical threshold value of $p_{th}$. Even though these threshold error rates
vary depending on the employed error model, the current consensus puts the
threshold at around $1\%$ \cite{terhal15} \cite{Versluis_2017}. However, we are
not aware of any experimental implementation capable of achieving such a low
error rate.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "QEC_paper"
%%% End: